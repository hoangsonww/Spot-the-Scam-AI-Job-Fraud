{
  "best_global_step": 2910,
  "best_metric": 0.08493854105472565,
  "best_model_checkpoint": "/home/snguyen/spot-the-scam-project/artifacts/transformer/checkpoint-2910",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 2910,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010309278350515464,
      "grad_norm": 4.317322254180908,
      "learning_rate": 9.278350515463919e-07,
      "loss": 0.6838,
      "step": 10
    },
    {
      "epoch": 0.020618556701030927,
      "grad_norm": 3.7847299575805664,
      "learning_rate": 1.9587628865979384e-06,
      "loss": 0.6513,
      "step": 20
    },
    {
      "epoch": 0.030927835051546393,
      "grad_norm": 4.014113426208496,
      "learning_rate": 2.989690721649485e-06,
      "loss": 0.6006,
      "step": 30
    },
    {
      "epoch": 0.041237113402061855,
      "grad_norm": 3.813490152359009,
      "learning_rate": 4.020618556701031e-06,
      "loss": 0.5166,
      "step": 40
    },
    {
      "epoch": 0.05154639175257732,
      "grad_norm": 3.2401645183563232,
      "learning_rate": 5.051546391752578e-06,
      "loss": 0.4146,
      "step": 50
    },
    {
      "epoch": 0.061855670103092786,
      "grad_norm": 2.0531139373779297,
      "learning_rate": 6.082474226804124e-06,
      "loss": 0.2646,
      "step": 60
    },
    {
      "epoch": 0.07216494845360824,
      "grad_norm": 1.1098695993423462,
      "learning_rate": 7.11340206185567e-06,
      "loss": 0.2111,
      "step": 70
    },
    {
      "epoch": 0.08247422680412371,
      "grad_norm": 1.316699743270874,
      "learning_rate": 8.144329896907216e-06,
      "loss": 0.2271,
      "step": 80
    },
    {
      "epoch": 0.09278350515463918,
      "grad_norm": 0.9605346322059631,
      "learning_rate": 9.175257731958764e-06,
      "loss": 0.1704,
      "step": 90
    },
    {
      "epoch": 0.10309278350515463,
      "grad_norm": 0.6924274563789368,
      "learning_rate": 1.020618556701031e-05,
      "loss": 0.1418,
      "step": 100
    },
    {
      "epoch": 0.1134020618556701,
      "grad_norm": 2.1868643760681152,
      "learning_rate": 1.1237113402061856e-05,
      "loss": 0.2207,
      "step": 110
    },
    {
      "epoch": 0.12371134020618557,
      "grad_norm": 1.164217472076416,
      "learning_rate": 1.2268041237113401e-05,
      "loss": 0.088,
      "step": 120
    },
    {
      "epoch": 0.13402061855670103,
      "grad_norm": 0.3675380349159241,
      "learning_rate": 1.3298969072164948e-05,
      "loss": 0.1181,
      "step": 130
    },
    {
      "epoch": 0.14432989690721648,
      "grad_norm": 1.9823670387268066,
      "learning_rate": 1.4329896907216495e-05,
      "loss": 0.1558,
      "step": 140
    },
    {
      "epoch": 0.15463917525773196,
      "grad_norm": 1.5537714958190918,
      "learning_rate": 1.536082474226804e-05,
      "loss": 0.1945,
      "step": 150
    },
    {
      "epoch": 0.16494845360824742,
      "grad_norm": 1.1000179052352905,
      "learning_rate": 1.6391752577319588e-05,
      "loss": 0.1794,
      "step": 160
    },
    {
      "epoch": 0.17525773195876287,
      "grad_norm": 1.3513368368148804,
      "learning_rate": 1.7422680412371137e-05,
      "loss": 0.176,
      "step": 170
    },
    {
      "epoch": 0.18556701030927836,
      "grad_norm": 0.5899937748908997,
      "learning_rate": 1.8453608247422682e-05,
      "loss": 0.1248,
      "step": 180
    },
    {
      "epoch": 0.1958762886597938,
      "grad_norm": 1.948470950126648,
      "learning_rate": 1.9484536082474227e-05,
      "loss": 0.1509,
      "step": 190
    },
    {
      "epoch": 0.20618556701030927,
      "grad_norm": 3.1769511699676514,
      "learning_rate": 2.0515463917525773e-05,
      "loss": 0.1989,
      "step": 200
    },
    {
      "epoch": 0.21649484536082475,
      "grad_norm": 0.1754952371120453,
      "learning_rate": 2.154639175257732e-05,
      "loss": 0.0668,
      "step": 210
    },
    {
      "epoch": 0.2268041237113402,
      "grad_norm": 0.16015389561653137,
      "learning_rate": 2.2577319587628867e-05,
      "loss": 0.0669,
      "step": 220
    },
    {
      "epoch": 0.23711340206185566,
      "grad_norm": 1.3455579280853271,
      "learning_rate": 2.3608247422680412e-05,
      "loss": 0.2019,
      "step": 230
    },
    {
      "epoch": 0.24742268041237114,
      "grad_norm": 1.5434907674789429,
      "learning_rate": 2.4639175257731957e-05,
      "loss": 0.1048,
      "step": 240
    },
    {
      "epoch": 0.25773195876288657,
      "grad_norm": 2.4456286430358887,
      "learning_rate": 2.5670103092783506e-05,
      "loss": 0.3169,
      "step": 250
    },
    {
      "epoch": 0.26804123711340205,
      "grad_norm": 0.8179097175598145,
      "learning_rate": 2.670103092783505e-05,
      "loss": 0.1462,
      "step": 260
    },
    {
      "epoch": 0.27835051546391754,
      "grad_norm": 2.0811967849731445,
      "learning_rate": 2.77319587628866e-05,
      "loss": 0.2081,
      "step": 270
    },
    {
      "epoch": 0.28865979381443296,
      "grad_norm": 2.03605055809021,
      "learning_rate": 2.8762886597938146e-05,
      "loss": 0.2851,
      "step": 280
    },
    {
      "epoch": 0.29896907216494845,
      "grad_norm": 4.211353778839111,
      "learning_rate": 2.979381443298969e-05,
      "loss": 0.195,
      "step": 290
    },
    {
      "epoch": 0.30927835051546393,
      "grad_norm": 0.8008865714073181,
      "learning_rate": 2.990836197021764e-05,
      "loss": 0.2742,
      "step": 300
    },
    {
      "epoch": 0.31958762886597936,
      "grad_norm": 0.48727262020111084,
      "learning_rate": 2.979381443298969e-05,
      "loss": 0.1458,
      "step": 310
    },
    {
      "epoch": 0.32989690721649484,
      "grad_norm": 0.18909798562526703,
      "learning_rate": 2.9679266895761742e-05,
      "loss": 0.162,
      "step": 320
    },
    {
      "epoch": 0.3402061855670103,
      "grad_norm": 0.41817986965179443,
      "learning_rate": 2.9564719358533794e-05,
      "loss": 0.197,
      "step": 330
    },
    {
      "epoch": 0.35051546391752575,
      "grad_norm": 1.103466272354126,
      "learning_rate": 2.945017182130584e-05,
      "loss": 0.1832,
      "step": 340
    },
    {
      "epoch": 0.36082474226804123,
      "grad_norm": 0.43730297684669495,
      "learning_rate": 2.9335624284077893e-05,
      "loss": 0.1092,
      "step": 350
    },
    {
      "epoch": 0.3711340206185567,
      "grad_norm": 1.3533095121383667,
      "learning_rate": 2.9221076746849944e-05,
      "loss": 0.0788,
      "step": 360
    },
    {
      "epoch": 0.38144329896907214,
      "grad_norm": 1.051343321800232,
      "learning_rate": 2.9106529209621995e-05,
      "loss": 0.0889,
      "step": 370
    },
    {
      "epoch": 0.3917525773195876,
      "grad_norm": 2.657198429107666,
      "learning_rate": 2.8991981672394047e-05,
      "loss": 0.1663,
      "step": 380
    },
    {
      "epoch": 0.4020618556701031,
      "grad_norm": 1.4721581935882568,
      "learning_rate": 2.8877434135166094e-05,
      "loss": 0.1642,
      "step": 390
    },
    {
      "epoch": 0.41237113402061853,
      "grad_norm": 4.815724849700928,
      "learning_rate": 2.8762886597938146e-05,
      "loss": 0.0965,
      "step": 400
    },
    {
      "epoch": 0.422680412371134,
      "grad_norm": 0.3778661787509918,
      "learning_rate": 2.8648339060710194e-05,
      "loss": 0.1453,
      "step": 410
    },
    {
      "epoch": 0.4329896907216495,
      "grad_norm": 0.39545783400535583,
      "learning_rate": 2.8533791523482248e-05,
      "loss": 0.1116,
      "step": 420
    },
    {
      "epoch": 0.44329896907216493,
      "grad_norm": 3.3261289596557617,
      "learning_rate": 2.8419243986254296e-05,
      "loss": 0.1313,
      "step": 430
    },
    {
      "epoch": 0.4536082474226804,
      "grad_norm": 0.28030434250831604,
      "learning_rate": 2.8304696449026347e-05,
      "loss": 0.1352,
      "step": 440
    },
    {
      "epoch": 0.4639175257731959,
      "grad_norm": 3.1026768684387207,
      "learning_rate": 2.8190148911798395e-05,
      "loss": 0.1893,
      "step": 450
    },
    {
      "epoch": 0.4742268041237113,
      "grad_norm": 0.4618614912033081,
      "learning_rate": 2.8075601374570446e-05,
      "loss": 0.1226,
      "step": 460
    },
    {
      "epoch": 0.4845360824742268,
      "grad_norm": 0.3848640024662018,
      "learning_rate": 2.7961053837342498e-05,
      "loss": 0.1057,
      "step": 470
    },
    {
      "epoch": 0.4948453608247423,
      "grad_norm": 1.8427672386169434,
      "learning_rate": 2.784650630011455e-05,
      "loss": 0.0627,
      "step": 480
    },
    {
      "epoch": 0.5051546391752577,
      "grad_norm": 0.1455574482679367,
      "learning_rate": 2.77319587628866e-05,
      "loss": 0.0561,
      "step": 490
    },
    {
      "epoch": 0.5154639175257731,
      "grad_norm": 1.9626494646072388,
      "learning_rate": 2.7617411225658648e-05,
      "loss": 0.0704,
      "step": 500
    },
    {
      "epoch": 0.5257731958762887,
      "grad_norm": 0.1372571736574173,
      "learning_rate": 2.75028636884307e-05,
      "loss": 0.083,
      "step": 510
    },
    {
      "epoch": 0.5360824742268041,
      "grad_norm": 0.415608286857605,
      "learning_rate": 2.738831615120275e-05,
      "loss": 0.154,
      "step": 520
    },
    {
      "epoch": 0.5463917525773195,
      "grad_norm": 2.4921343326568604,
      "learning_rate": 2.7273768613974802e-05,
      "loss": 0.1529,
      "step": 530
    },
    {
      "epoch": 0.5567010309278351,
      "grad_norm": 1.7764369249343872,
      "learning_rate": 2.715922107674685e-05,
      "loss": 0.1447,
      "step": 540
    },
    {
      "epoch": 0.5670103092783505,
      "grad_norm": 1.8318613767623901,
      "learning_rate": 2.70446735395189e-05,
      "loss": 0.0953,
      "step": 550
    },
    {
      "epoch": 0.5773195876288659,
      "grad_norm": 0.20844320952892303,
      "learning_rate": 2.693012600229095e-05,
      "loss": 0.0838,
      "step": 560
    },
    {
      "epoch": 0.5876288659793815,
      "grad_norm": 0.5568075776100159,
      "learning_rate": 2.6815578465063004e-05,
      "loss": 0.0897,
      "step": 570
    },
    {
      "epoch": 0.5979381443298969,
      "grad_norm": 0.1765379011631012,
      "learning_rate": 2.670103092783505e-05,
      "loss": 0.1283,
      "step": 580
    },
    {
      "epoch": 0.6082474226804123,
      "grad_norm": 0.24604442715644836,
      "learning_rate": 2.6586483390607103e-05,
      "loss": 0.1264,
      "step": 590
    },
    {
      "epoch": 0.6185567010309279,
      "grad_norm": 1.9474186897277832,
      "learning_rate": 2.6471935853379154e-05,
      "loss": 0.0798,
      "step": 600
    },
    {
      "epoch": 0.6288659793814433,
      "grad_norm": 0.8986777663230896,
      "learning_rate": 2.6357388316151202e-05,
      "loss": 0.0913,
      "step": 610
    },
    {
      "epoch": 0.6391752577319587,
      "grad_norm": 2.4625489711761475,
      "learning_rate": 2.6242840778923257e-05,
      "loss": 0.0934,
      "step": 620
    },
    {
      "epoch": 0.6494845360824743,
      "grad_norm": 0.09236301481723785,
      "learning_rate": 2.6128293241695305e-05,
      "loss": 0.1008,
      "step": 630
    },
    {
      "epoch": 0.6597938144329897,
      "grad_norm": 3.6788411140441895,
      "learning_rate": 2.6013745704467356e-05,
      "loss": 0.1849,
      "step": 640
    },
    {
      "epoch": 0.6701030927835051,
      "grad_norm": 0.8317734003067017,
      "learning_rate": 2.5899198167239404e-05,
      "loss": 0.1202,
      "step": 650
    },
    {
      "epoch": 0.6804123711340206,
      "grad_norm": 0.20874406397342682,
      "learning_rate": 2.5784650630011455e-05,
      "loss": 0.1303,
      "step": 660
    },
    {
      "epoch": 0.6907216494845361,
      "grad_norm": 0.15031133592128754,
      "learning_rate": 2.5670103092783506e-05,
      "loss": 0.0775,
      "step": 670
    },
    {
      "epoch": 0.7010309278350515,
      "grad_norm": 2.3510255813598633,
      "learning_rate": 2.5555555555555557e-05,
      "loss": 0.1774,
      "step": 680
    },
    {
      "epoch": 0.711340206185567,
      "grad_norm": 0.22607724368572235,
      "learning_rate": 2.5441008018327605e-05,
      "loss": 0.0978,
      "step": 690
    },
    {
      "epoch": 0.7216494845360825,
      "grad_norm": 0.1604790836572647,
      "learning_rate": 2.5326460481099657e-05,
      "loss": 0.0601,
      "step": 700
    },
    {
      "epoch": 0.7319587628865979,
      "grad_norm": 3.4016976356506348,
      "learning_rate": 2.5211912943871708e-05,
      "loss": 0.1612,
      "step": 710
    },
    {
      "epoch": 0.7422680412371134,
      "grad_norm": 1.6675087213516235,
      "learning_rate": 2.5097365406643756e-05,
      "loss": 0.1011,
      "step": 720
    },
    {
      "epoch": 0.7525773195876289,
      "grad_norm": 0.999332845211029,
      "learning_rate": 2.498281786941581e-05,
      "loss": 0.1471,
      "step": 730
    },
    {
      "epoch": 0.7628865979381443,
      "grad_norm": 0.1117347702383995,
      "learning_rate": 2.4868270332187858e-05,
      "loss": 0.0988,
      "step": 740
    },
    {
      "epoch": 0.7731958762886598,
      "grad_norm": 0.07534588128328323,
      "learning_rate": 2.475372279495991e-05,
      "loss": 0.0523,
      "step": 750
    },
    {
      "epoch": 0.7835051546391752,
      "grad_norm": 0.0825510025024414,
      "learning_rate": 2.4639175257731957e-05,
      "loss": 0.0884,
      "step": 760
    },
    {
      "epoch": 0.7938144329896907,
      "grad_norm": 0.15976516902446747,
      "learning_rate": 2.452462772050401e-05,
      "loss": 0.0964,
      "step": 770
    },
    {
      "epoch": 0.8041237113402062,
      "grad_norm": 0.983955442905426,
      "learning_rate": 2.441008018327606e-05,
      "loss": 0.0932,
      "step": 780
    },
    {
      "epoch": 0.8144329896907216,
      "grad_norm": 4.274669647216797,
      "learning_rate": 2.429553264604811e-05,
      "loss": 0.0888,
      "step": 790
    },
    {
      "epoch": 0.8247422680412371,
      "grad_norm": 0.08123548328876495,
      "learning_rate": 2.418098510882016e-05,
      "loss": 0.1129,
      "step": 800
    },
    {
      "epoch": 0.8350515463917526,
      "grad_norm": 3.4843790531158447,
      "learning_rate": 2.406643757159221e-05,
      "loss": 0.1483,
      "step": 810
    },
    {
      "epoch": 0.845360824742268,
      "grad_norm": 3.758626699447632,
      "learning_rate": 2.395189003436426e-05,
      "loss": 0.1841,
      "step": 820
    },
    {
      "epoch": 0.8556701030927835,
      "grad_norm": 0.23663321137428284,
      "learning_rate": 2.3837342497136313e-05,
      "loss": 0.0931,
      "step": 830
    },
    {
      "epoch": 0.865979381443299,
      "grad_norm": 2.18538498878479,
      "learning_rate": 2.3722794959908364e-05,
      "loss": 0.1594,
      "step": 840
    },
    {
      "epoch": 0.8762886597938144,
      "grad_norm": 0.9172095656394958,
      "learning_rate": 2.3608247422680412e-05,
      "loss": 0.0685,
      "step": 850
    },
    {
      "epoch": 0.8865979381443299,
      "grad_norm": 1.8701672554016113,
      "learning_rate": 2.3493699885452463e-05,
      "loss": 0.1037,
      "step": 860
    },
    {
      "epoch": 0.8969072164948454,
      "grad_norm": 2.337604522705078,
      "learning_rate": 2.337915234822451e-05,
      "loss": 0.151,
      "step": 870
    },
    {
      "epoch": 0.9072164948453608,
      "grad_norm": 2.5061404705047607,
      "learning_rate": 2.3264604810996566e-05,
      "loss": 0.0926,
      "step": 880
    },
    {
      "epoch": 0.9175257731958762,
      "grad_norm": 0.431869775056839,
      "learning_rate": 2.3150057273768614e-05,
      "loss": 0.1028,
      "step": 890
    },
    {
      "epoch": 0.9278350515463918,
      "grad_norm": 1.6937427520751953,
      "learning_rate": 2.3035509736540665e-05,
      "loss": 0.1661,
      "step": 900
    },
    {
      "epoch": 0.9381443298969072,
      "grad_norm": 2.270498037338257,
      "learning_rate": 2.2920962199312713e-05,
      "loss": 0.1944,
      "step": 910
    },
    {
      "epoch": 0.9484536082474226,
      "grad_norm": 2.4242637157440186,
      "learning_rate": 2.2806414662084764e-05,
      "loss": 0.1574,
      "step": 920
    },
    {
      "epoch": 0.9587628865979382,
      "grad_norm": 0.25543248653411865,
      "learning_rate": 2.269186712485682e-05,
      "loss": 0.126,
      "step": 930
    },
    {
      "epoch": 0.9690721649484536,
      "grad_norm": 1.9672725200653076,
      "learning_rate": 2.2577319587628867e-05,
      "loss": 0.09,
      "step": 940
    },
    {
      "epoch": 0.979381443298969,
      "grad_norm": 0.4833912253379822,
      "learning_rate": 2.2462772050400918e-05,
      "loss": 0.1557,
      "step": 950
    },
    {
      "epoch": 0.9896907216494846,
      "grad_norm": 0.22959625720977783,
      "learning_rate": 2.2348224513172966e-05,
      "loss": 0.0307,
      "step": 960
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.08184825628995895,
      "learning_rate": 2.2233676975945017e-05,
      "loss": 0.1052,
      "step": 970
    },
    {
      "epoch": 1.0,
      "eval_f1": 0.6255506607929515,
      "eval_loss": 0.10399597138166428,
      "eval_precision": 0.922077922077922,
      "eval_recall": 0.47333333333333333,
      "eval_runtime": 2.0961,
      "eval_samples_per_second": 1585.299,
      "eval_steps_per_second": 99.23,
      "step": 970
    },
    {
      "epoch": 1.0103092783505154,
      "grad_norm": 1.5995287895202637,
      "learning_rate": 2.211912943871707e-05,
      "loss": 0.121,
      "step": 980
    },
    {
      "epoch": 1.0206185567010309,
      "grad_norm": 0.15549692511558533,
      "learning_rate": 2.200458190148912e-05,
      "loss": 0.093,
      "step": 990
    },
    {
      "epoch": 1.0309278350515463,
      "grad_norm": 0.472072958946228,
      "learning_rate": 2.1890034364261168e-05,
      "loss": 0.0706,
      "step": 1000
    },
    {
      "epoch": 1.041237113402062,
      "grad_norm": 0.2633991837501526,
      "learning_rate": 2.177548682703322e-05,
      "loss": 0.0802,
      "step": 1010
    },
    {
      "epoch": 1.0515463917525774,
      "grad_norm": 0.10138984024524689,
      "learning_rate": 2.1660939289805267e-05,
      "loss": 0.0205,
      "step": 1020
    },
    {
      "epoch": 1.0618556701030928,
      "grad_norm": 2.4572231769561768,
      "learning_rate": 2.154639175257732e-05,
      "loss": 0.0755,
      "step": 1030
    },
    {
      "epoch": 1.0721649484536082,
      "grad_norm": 0.22678345441818237,
      "learning_rate": 2.1431844215349373e-05,
      "loss": 0.0167,
      "step": 1040
    },
    {
      "epoch": 1.0824742268041236,
      "grad_norm": 0.053279805928468704,
      "learning_rate": 2.131729667812142e-05,
      "loss": 0.0701,
      "step": 1050
    },
    {
      "epoch": 1.0927835051546393,
      "grad_norm": 0.10935678333044052,
      "learning_rate": 2.1202749140893472e-05,
      "loss": 0.0945,
      "step": 1060
    },
    {
      "epoch": 1.1030927835051547,
      "grad_norm": 0.22190889716148376,
      "learning_rate": 2.108820160366552e-05,
      "loss": 0.0074,
      "step": 1070
    },
    {
      "epoch": 1.1134020618556701,
      "grad_norm": 1.5911173820495605,
      "learning_rate": 2.0973654066437574e-05,
      "loss": 0.0526,
      "step": 1080
    },
    {
      "epoch": 1.1237113402061856,
      "grad_norm": 0.05594988912343979,
      "learning_rate": 2.0859106529209622e-05,
      "loss": 0.0165,
      "step": 1090
    },
    {
      "epoch": 1.134020618556701,
      "grad_norm": 0.055130310356616974,
      "learning_rate": 2.0744558991981673e-05,
      "loss": 0.0875,
      "step": 1100
    },
    {
      "epoch": 1.1443298969072164,
      "grad_norm": 0.061669234186410904,
      "learning_rate": 2.063001145475372e-05,
      "loss": 0.0217,
      "step": 1110
    },
    {
      "epoch": 1.1546391752577319,
      "grad_norm": 0.12843729555606842,
      "learning_rate": 2.0515463917525773e-05,
      "loss": 0.0074,
      "step": 1120
    },
    {
      "epoch": 1.1649484536082475,
      "grad_norm": 0.5610121488571167,
      "learning_rate": 2.0400916380297824e-05,
      "loss": 0.1099,
      "step": 1130
    },
    {
      "epoch": 1.175257731958763,
      "grad_norm": 0.1805131882429123,
      "learning_rate": 2.0286368843069875e-05,
      "loss": 0.1283,
      "step": 1140
    },
    {
      "epoch": 1.1855670103092784,
      "grad_norm": 0.37798914313316345,
      "learning_rate": 2.0171821305841926e-05,
      "loss": 0.0528,
      "step": 1150
    },
    {
      "epoch": 1.1958762886597938,
      "grad_norm": 0.0681704729795456,
      "learning_rate": 2.0057273768613974e-05,
      "loss": 0.0917,
      "step": 1160
    },
    {
      "epoch": 1.2061855670103092,
      "grad_norm": 7.151482105255127,
      "learning_rate": 1.9942726231386026e-05,
      "loss": 0.128,
      "step": 1170
    },
    {
      "epoch": 1.2164948453608249,
      "grad_norm": 0.13324975967407227,
      "learning_rate": 1.9828178694158077e-05,
      "loss": 0.0485,
      "step": 1180
    },
    {
      "epoch": 1.2268041237113403,
      "grad_norm": 0.268785685300827,
      "learning_rate": 1.9713631156930128e-05,
      "loss": 0.0493,
      "step": 1190
    },
    {
      "epoch": 1.2371134020618557,
      "grad_norm": 0.15462301671504974,
      "learning_rate": 1.9599083619702176e-05,
      "loss": 0.024,
      "step": 1200
    },
    {
      "epoch": 1.2474226804123711,
      "grad_norm": 0.25126659870147705,
      "learning_rate": 1.9484536082474227e-05,
      "loss": 0.1145,
      "step": 1210
    },
    {
      "epoch": 1.2577319587628866,
      "grad_norm": 0.17445459961891174,
      "learning_rate": 1.9369988545246275e-05,
      "loss": 0.0356,
      "step": 1220
    },
    {
      "epoch": 1.268041237113402,
      "grad_norm": 7.5428314208984375,
      "learning_rate": 1.925544100801833e-05,
      "loss": 0.0483,
      "step": 1230
    },
    {
      "epoch": 1.2783505154639174,
      "grad_norm": 0.10194358229637146,
      "learning_rate": 1.9140893470790378e-05,
      "loss": 0.0028,
      "step": 1240
    },
    {
      "epoch": 1.2886597938144329,
      "grad_norm": 0.024999350309371948,
      "learning_rate": 1.902634593356243e-05,
      "loss": 0.0216,
      "step": 1250
    },
    {
      "epoch": 1.2989690721649485,
      "grad_norm": 0.35449379682540894,
      "learning_rate": 1.891179839633448e-05,
      "loss": 0.0514,
      "step": 1260
    },
    {
      "epoch": 1.309278350515464,
      "grad_norm": 0.04144033417105675,
      "learning_rate": 1.8797250859106528e-05,
      "loss": 0.1115,
      "step": 1270
    },
    {
      "epoch": 1.3195876288659794,
      "grad_norm": 5.993964195251465,
      "learning_rate": 1.8682703321878583e-05,
      "loss": 0.1158,
      "step": 1280
    },
    {
      "epoch": 1.3298969072164948,
      "grad_norm": 0.193878635764122,
      "learning_rate": 1.856815578465063e-05,
      "loss": 0.0332,
      "step": 1290
    },
    {
      "epoch": 1.3402061855670104,
      "grad_norm": 5.7554779052734375,
      "learning_rate": 1.8453608247422682e-05,
      "loss": 0.0452,
      "step": 1300
    },
    {
      "epoch": 1.3505154639175259,
      "grad_norm": 0.29848095774650574,
      "learning_rate": 1.833906071019473e-05,
      "loss": 0.0173,
      "step": 1310
    },
    {
      "epoch": 1.3608247422680413,
      "grad_norm": 0.03686927631497383,
      "learning_rate": 1.822451317296678e-05,
      "loss": 0.0731,
      "step": 1320
    },
    {
      "epoch": 1.3711340206185567,
      "grad_norm": 3.993098497390747,
      "learning_rate": 1.8109965635738832e-05,
      "loss": 0.1721,
      "step": 1330
    },
    {
      "epoch": 1.3814432989690721,
      "grad_norm": 0.09406793862581253,
      "learning_rate": 1.7995418098510884e-05,
      "loss": 0.0287,
      "step": 1340
    },
    {
      "epoch": 1.3917525773195876,
      "grad_norm": 4.414337158203125,
      "learning_rate": 1.788087056128293e-05,
      "loss": 0.1123,
      "step": 1350
    },
    {
      "epoch": 1.402061855670103,
      "grad_norm": 2.9138755798339844,
      "learning_rate": 1.7766323024054983e-05,
      "loss": 0.0446,
      "step": 1360
    },
    {
      "epoch": 1.4123711340206184,
      "grad_norm": 3.700578212738037,
      "learning_rate": 1.765177548682703e-05,
      "loss": 0.1632,
      "step": 1370
    },
    {
      "epoch": 1.422680412371134,
      "grad_norm": 11.481321334838867,
      "learning_rate": 1.7537227949599085e-05,
      "loss": 0.1213,
      "step": 1380
    },
    {
      "epoch": 1.4329896907216495,
      "grad_norm": 1.5375173091888428,
      "learning_rate": 1.7422680412371137e-05,
      "loss": 0.0697,
      "step": 1390
    },
    {
      "epoch": 1.443298969072165,
      "grad_norm": 8.990697860717773,
      "learning_rate": 1.7308132875143184e-05,
      "loss": 0.1209,
      "step": 1400
    },
    {
      "epoch": 1.4536082474226804,
      "grad_norm": 0.4084581136703491,
      "learning_rate": 1.7193585337915236e-05,
      "loss": 0.0304,
      "step": 1410
    },
    {
      "epoch": 1.463917525773196,
      "grad_norm": 7.889587879180908,
      "learning_rate": 1.7079037800687284e-05,
      "loss": 0.0831,
      "step": 1420
    },
    {
      "epoch": 1.4742268041237114,
      "grad_norm": 4.947115898132324,
      "learning_rate": 1.6964490263459338e-05,
      "loss": 0.0514,
      "step": 1430
    },
    {
      "epoch": 1.4845360824742269,
      "grad_norm": 0.12064512073993683,
      "learning_rate": 1.6849942726231386e-05,
      "loss": 0.1063,
      "step": 1440
    },
    {
      "epoch": 1.4948453608247423,
      "grad_norm": 0.1757403165102005,
      "learning_rate": 1.6735395189003437e-05,
      "loss": 0.136,
      "step": 1450
    },
    {
      "epoch": 1.5051546391752577,
      "grad_norm": 2.8000292778015137,
      "learning_rate": 1.6620847651775485e-05,
      "loss": 0.0887,
      "step": 1460
    },
    {
      "epoch": 1.5154639175257731,
      "grad_norm": 0.17659245431423187,
      "learning_rate": 1.6506300114547537e-05,
      "loss": 0.064,
      "step": 1470
    },
    {
      "epoch": 1.5257731958762886,
      "grad_norm": 0.14342640340328217,
      "learning_rate": 1.6391752577319588e-05,
      "loss": 0.0154,
      "step": 1480
    },
    {
      "epoch": 1.536082474226804,
      "grad_norm": 0.07562953233718872,
      "learning_rate": 1.627720504009164e-05,
      "loss": 0.0177,
      "step": 1490
    },
    {
      "epoch": 1.5463917525773194,
      "grad_norm": 0.03069681115448475,
      "learning_rate": 1.616265750286369e-05,
      "loss": 0.0227,
      "step": 1500
    },
    {
      "epoch": 1.556701030927835,
      "grad_norm": 1.0728269815444946,
      "learning_rate": 1.6048109965635738e-05,
      "loss": 0.0476,
      "step": 1510
    },
    {
      "epoch": 1.5670103092783505,
      "grad_norm": 0.15437418222427368,
      "learning_rate": 1.593356242840779e-05,
      "loss": 0.045,
      "step": 1520
    },
    {
      "epoch": 1.577319587628866,
      "grad_norm": 0.15484167635440826,
      "learning_rate": 1.581901489117984e-05,
      "loss": 0.0623,
      "step": 1530
    },
    {
      "epoch": 1.5876288659793816,
      "grad_norm": 10.193100929260254,
      "learning_rate": 1.5704467353951892e-05,
      "loss": 0.1002,
      "step": 1540
    },
    {
      "epoch": 1.597938144329897,
      "grad_norm": 0.14705206453800201,
      "learning_rate": 1.558991981672394e-05,
      "loss": 0.0564,
      "step": 1550
    },
    {
      "epoch": 1.6082474226804124,
      "grad_norm": 0.031220123171806335,
      "learning_rate": 1.547537227949599e-05,
      "loss": 0.062,
      "step": 1560
    },
    {
      "epoch": 1.6185567010309279,
      "grad_norm": 13.10914134979248,
      "learning_rate": 1.536082474226804e-05,
      "loss": 0.056,
      "step": 1570
    },
    {
      "epoch": 1.6288659793814433,
      "grad_norm": 2.6585564613342285,
      "learning_rate": 1.5246277205040092e-05,
      "loss": 0.0554,
      "step": 1580
    },
    {
      "epoch": 1.6391752577319587,
      "grad_norm": 2.6038424968719482,
      "learning_rate": 1.5131729667812142e-05,
      "loss": 0.0625,
      "step": 1590
    },
    {
      "epoch": 1.6494845360824741,
      "grad_norm": 15.812728881835938,
      "learning_rate": 1.5017182130584193e-05,
      "loss": 0.0618,
      "step": 1600
    },
    {
      "epoch": 1.6597938144329896,
      "grad_norm": 0.06261929869651794,
      "learning_rate": 1.4902634593356242e-05,
      "loss": 0.0926,
      "step": 1610
    },
    {
      "epoch": 1.670103092783505,
      "grad_norm": 0.03720169886946678,
      "learning_rate": 1.4788087056128294e-05,
      "loss": 0.0493,
      "step": 1620
    },
    {
      "epoch": 1.6804123711340206,
      "grad_norm": 0.27274858951568604,
      "learning_rate": 1.4673539518900343e-05,
      "loss": 0.1066,
      "step": 1630
    },
    {
      "epoch": 1.690721649484536,
      "grad_norm": 0.2451740801334381,
      "learning_rate": 1.4558991981672395e-05,
      "loss": 0.0662,
      "step": 1640
    },
    {
      "epoch": 1.7010309278350515,
      "grad_norm": 0.06481938809156418,
      "learning_rate": 1.4444444444444444e-05,
      "loss": 0.0175,
      "step": 1650
    },
    {
      "epoch": 1.7113402061855671,
      "grad_norm": 0.3551510274410248,
      "learning_rate": 1.4329896907216495e-05,
      "loss": 0.025,
      "step": 1660
    },
    {
      "epoch": 1.7216494845360826,
      "grad_norm": 3.454110860824585,
      "learning_rate": 1.4215349369988547e-05,
      "loss": 0.0491,
      "step": 1670
    },
    {
      "epoch": 1.731958762886598,
      "grad_norm": 4.7935566902160645,
      "learning_rate": 1.4100801832760596e-05,
      "loss": 0.1002,
      "step": 1680
    },
    {
      "epoch": 1.7422680412371134,
      "grad_norm": 3.7329447269439697,
      "learning_rate": 1.3986254295532648e-05,
      "loss": 0.0502,
      "step": 1690
    },
    {
      "epoch": 1.7525773195876289,
      "grad_norm": 1.8606517314910889,
      "learning_rate": 1.3871706758304697e-05,
      "loss": 0.0671,
      "step": 1700
    },
    {
      "epoch": 1.7628865979381443,
      "grad_norm": 0.034814268350601196,
      "learning_rate": 1.3757159221076747e-05,
      "loss": 0.1198,
      "step": 1710
    },
    {
      "epoch": 1.7731958762886597,
      "grad_norm": 0.060236793011426926,
      "learning_rate": 1.3642611683848798e-05,
      "loss": 0.0674,
      "step": 1720
    },
    {
      "epoch": 1.7835051546391751,
      "grad_norm": 14.00594711303711,
      "learning_rate": 1.3528064146620847e-05,
      "loss": 0.0211,
      "step": 1730
    },
    {
      "epoch": 1.7938144329896906,
      "grad_norm": 0.46076700091362,
      "learning_rate": 1.3413516609392899e-05,
      "loss": 0.0048,
      "step": 1740
    },
    {
      "epoch": 1.8041237113402062,
      "grad_norm": 0.037658434361219406,
      "learning_rate": 1.3298969072164948e-05,
      "loss": 0.0463,
      "step": 1750
    },
    {
      "epoch": 1.8144329896907216,
      "grad_norm": 1.3553987741470337,
      "learning_rate": 1.3184421534936998e-05,
      "loss": 0.03,
      "step": 1760
    },
    {
      "epoch": 1.824742268041237,
      "grad_norm": 0.02495512366294861,
      "learning_rate": 1.3069873997709051e-05,
      "loss": 0.0461,
      "step": 1770
    },
    {
      "epoch": 1.8350515463917527,
      "grad_norm": 0.04467793181538582,
      "learning_rate": 1.29553264604811e-05,
      "loss": 0.0218,
      "step": 1780
    },
    {
      "epoch": 1.8453608247422681,
      "grad_norm": 0.027244575321674347,
      "learning_rate": 1.284077892325315e-05,
      "loss": 0.0792,
      "step": 1790
    },
    {
      "epoch": 1.8556701030927836,
      "grad_norm": 0.05541550740599632,
      "learning_rate": 1.2726231386025201e-05,
      "loss": 0.0798,
      "step": 1800
    },
    {
      "epoch": 1.865979381443299,
      "grad_norm": 0.5655286908149719,
      "learning_rate": 1.2611683848797251e-05,
      "loss": 0.0832,
      "step": 1810
    },
    {
      "epoch": 1.8762886597938144,
      "grad_norm": 18.85749626159668,
      "learning_rate": 1.2497136311569302e-05,
      "loss": 0.086,
      "step": 1820
    },
    {
      "epoch": 1.8865979381443299,
      "grad_norm": 0.21951572597026825,
      "learning_rate": 1.2382588774341352e-05,
      "loss": 0.0038,
      "step": 1830
    },
    {
      "epoch": 1.8969072164948453,
      "grad_norm": 0.02487076260149479,
      "learning_rate": 1.2268041237113401e-05,
      "loss": 0.0388,
      "step": 1840
    },
    {
      "epoch": 1.9072164948453607,
      "grad_norm": 0.25477856397628784,
      "learning_rate": 1.2153493699885453e-05,
      "loss": 0.0317,
      "step": 1850
    },
    {
      "epoch": 1.9175257731958761,
      "grad_norm": 2.4845657348632812,
      "learning_rate": 1.2038946162657502e-05,
      "loss": 0.099,
      "step": 1860
    },
    {
      "epoch": 1.9278350515463918,
      "grad_norm": 0.4771482050418854,
      "learning_rate": 1.1924398625429553e-05,
      "loss": 0.0575,
      "step": 1870
    },
    {
      "epoch": 1.9381443298969072,
      "grad_norm": 0.028410082682967186,
      "learning_rate": 1.1809851088201605e-05,
      "loss": 0.0463,
      "step": 1880
    },
    {
      "epoch": 1.9484536082474226,
      "grad_norm": 0.06694682687520981,
      "learning_rate": 1.1695303550973654e-05,
      "loss": 0.0686,
      "step": 1890
    },
    {
      "epoch": 1.9587628865979383,
      "grad_norm": 0.04966794699430466,
      "learning_rate": 1.1580756013745706e-05,
      "loss": 0.1036,
      "step": 1900
    },
    {
      "epoch": 1.9690721649484537,
      "grad_norm": 0.04646746814250946,
      "learning_rate": 1.1466208476517755e-05,
      "loss": 0.1199,
      "step": 1910
    },
    {
      "epoch": 1.9793814432989691,
      "grad_norm": 0.07797440141439438,
      "learning_rate": 1.1351660939289806e-05,
      "loss": 0.0687,
      "step": 1920
    },
    {
      "epoch": 1.9896907216494846,
      "grad_norm": 1.7290457487106323,
      "learning_rate": 1.1237113402061856e-05,
      "loss": 0.0729,
      "step": 1930
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.01759009249508381,
      "learning_rate": 1.1122565864833906e-05,
      "loss": 0.051,
      "step": 1940
    },
    {
      "epoch": 2.0,
      "eval_f1": 0.7114624505928854,
      "eval_loss": 0.08820119500160217,
      "eval_precision": 0.8737864077669902,
      "eval_recall": 0.6,
      "eval_runtime": 1.9501,
      "eval_samples_per_second": 1704.024,
      "eval_steps_per_second": 106.662,
      "step": 1940
    },
    {
      "epoch": 2.0103092783505154,
      "grad_norm": 0.05200715735554695,
      "learning_rate": 1.1008018327605957e-05,
      "loss": 0.0017,
      "step": 1950
    },
    {
      "epoch": 2.020618556701031,
      "grad_norm": 0.02545180730521679,
      "learning_rate": 1.0893470790378006e-05,
      "loss": 0.0294,
      "step": 1960
    },
    {
      "epoch": 2.0309278350515463,
      "grad_norm": 0.1336331069469452,
      "learning_rate": 1.0778923253150058e-05,
      "loss": 0.0479,
      "step": 1970
    },
    {
      "epoch": 2.0412371134020617,
      "grad_norm": 0.14143700897693634,
      "learning_rate": 1.0664375715922107e-05,
      "loss": 0.0123,
      "step": 1980
    },
    {
      "epoch": 2.051546391752577,
      "grad_norm": 7.4790449142456055,
      "learning_rate": 1.0549828178694158e-05,
      "loss": 0.0268,
      "step": 1990
    },
    {
      "epoch": 2.0618556701030926,
      "grad_norm": 3.5959439277648926,
      "learning_rate": 1.043528064146621e-05,
      "loss": 0.0492,
      "step": 2000
    },
    {
      "epoch": 2.0721649484536084,
      "grad_norm": 0.3772642910480499,
      "learning_rate": 1.032073310423826e-05,
      "loss": 0.0296,
      "step": 2010
    },
    {
      "epoch": 2.082474226804124,
      "grad_norm": 1.050385594367981,
      "learning_rate": 1.020618556701031e-05,
      "loss": 0.0246,
      "step": 2020
    },
    {
      "epoch": 2.0927835051546393,
      "grad_norm": 0.02859996072947979,
      "learning_rate": 1.009163802978236e-05,
      "loss": 0.0228,
      "step": 2030
    },
    {
      "epoch": 2.1030927835051547,
      "grad_norm": 0.20338726043701172,
      "learning_rate": 9.97709049255441e-06,
      "loss": 0.013,
      "step": 2040
    },
    {
      "epoch": 2.11340206185567,
      "grad_norm": 0.04193950816988945,
      "learning_rate": 9.862542955326461e-06,
      "loss": 0.0374,
      "step": 2050
    },
    {
      "epoch": 2.1237113402061856,
      "grad_norm": 1.5297861099243164,
      "learning_rate": 9.74799541809851e-06,
      "loss": 0.0478,
      "step": 2060
    },
    {
      "epoch": 2.134020618556701,
      "grad_norm": 0.022709965705871582,
      "learning_rate": 9.633447880870562e-06,
      "loss": 0.0096,
      "step": 2070
    },
    {
      "epoch": 2.1443298969072164,
      "grad_norm": 0.6220529675483704,
      "learning_rate": 9.518900343642611e-06,
      "loss": 0.0085,
      "step": 2080
    },
    {
      "epoch": 2.154639175257732,
      "grad_norm": 22.492172241210938,
      "learning_rate": 9.404352806414661e-06,
      "loss": 0.0137,
      "step": 2090
    },
    {
      "epoch": 2.1649484536082473,
      "grad_norm": 0.02972101978957653,
      "learning_rate": 9.289805269186712e-06,
      "loss": 0.0422,
      "step": 2100
    },
    {
      "epoch": 2.1752577319587627,
      "grad_norm": 0.029633795842528343,
      "learning_rate": 9.175257731958764e-06,
      "loss": 0.04,
      "step": 2110
    },
    {
      "epoch": 2.1855670103092786,
      "grad_norm": 0.018952732905745506,
      "learning_rate": 9.060710194730815e-06,
      "loss": 0.0219,
      "step": 2120
    },
    {
      "epoch": 2.195876288659794,
      "grad_norm": 0.056438662111759186,
      "learning_rate": 8.946162657502864e-06,
      "loss": 0.1054,
      "step": 2130
    },
    {
      "epoch": 2.2061855670103094,
      "grad_norm": 0.043012835085392,
      "learning_rate": 8.831615120274914e-06,
      "loss": 0.0022,
      "step": 2140
    },
    {
      "epoch": 2.216494845360825,
      "grad_norm": 2.566312313079834,
      "learning_rate": 8.717067583046965e-06,
      "loss": 0.0127,
      "step": 2150
    },
    {
      "epoch": 2.2268041237113403,
      "grad_norm": 0.01937289535999298,
      "learning_rate": 8.602520045819015e-06,
      "loss": 0.0225,
      "step": 2160
    },
    {
      "epoch": 2.2371134020618557,
      "grad_norm": 7.853673458099365,
      "learning_rate": 8.487972508591066e-06,
      "loss": 0.0846,
      "step": 2170
    },
    {
      "epoch": 2.247422680412371,
      "grad_norm": 0.022290177643299103,
      "learning_rate": 8.373424971363116e-06,
      "loss": 0.0075,
      "step": 2180
    },
    {
      "epoch": 2.2577319587628866,
      "grad_norm": 1.0038260221481323,
      "learning_rate": 8.258877434135165e-06,
      "loss": 0.0112,
      "step": 2190
    },
    {
      "epoch": 2.268041237113402,
      "grad_norm": 0.07669180631637573,
      "learning_rate": 8.144329896907216e-06,
      "loss": 0.0159,
      "step": 2200
    },
    {
      "epoch": 2.2783505154639174,
      "grad_norm": 0.021386627107858658,
      "learning_rate": 8.029782359679266e-06,
      "loss": 0.0252,
      "step": 2210
    },
    {
      "epoch": 2.288659793814433,
      "grad_norm": 0.016966896131634712,
      "learning_rate": 7.915234822451319e-06,
      "loss": 0.0364,
      "step": 2220
    },
    {
      "epoch": 2.2989690721649483,
      "grad_norm": 0.16849175095558167,
      "learning_rate": 7.800687285223369e-06,
      "loss": 0.0011,
      "step": 2230
    },
    {
      "epoch": 2.3092783505154637,
      "grad_norm": 0.061844151467084885,
      "learning_rate": 7.686139747995418e-06,
      "loss": 0.0919,
      "step": 2240
    },
    {
      "epoch": 2.319587628865979,
      "grad_norm": 0.05223185941576958,
      "learning_rate": 7.5715922107674686e-06,
      "loss": 0.0153,
      "step": 2250
    },
    {
      "epoch": 2.329896907216495,
      "grad_norm": 5.368727684020996,
      "learning_rate": 7.457044673539519e-06,
      "loss": 0.0282,
      "step": 2260
    },
    {
      "epoch": 2.3402061855670104,
      "grad_norm": 0.035918228328228,
      "learning_rate": 7.342497136311569e-06,
      "loss": 0.0035,
      "step": 2270
    },
    {
      "epoch": 2.350515463917526,
      "grad_norm": 0.01461124885827303,
      "learning_rate": 7.22794959908362e-06,
      "loss": 0.0214,
      "step": 2280
    },
    {
      "epoch": 2.3608247422680413,
      "grad_norm": 0.01585947535932064,
      "learning_rate": 7.11340206185567e-06,
      "loss": 0.0836,
      "step": 2290
    },
    {
      "epoch": 2.3711340206185567,
      "grad_norm": 0.11010655015707016,
      "learning_rate": 6.998854524627721e-06,
      "loss": 0.06,
      "step": 2300
    },
    {
      "epoch": 2.381443298969072,
      "grad_norm": 0.07063210755586624,
      "learning_rate": 6.884306987399771e-06,
      "loss": 0.0475,
      "step": 2310
    },
    {
      "epoch": 2.3917525773195876,
      "grad_norm": 0.03172813728451729,
      "learning_rate": 6.7697594501718215e-06,
      "loss": 0.0223,
      "step": 2320
    },
    {
      "epoch": 2.402061855670103,
      "grad_norm": 0.2105456441640854,
      "learning_rate": 6.655211912943872e-06,
      "loss": 0.0159,
      "step": 2330
    },
    {
      "epoch": 2.4123711340206184,
      "grad_norm": 0.03088107332587242,
      "learning_rate": 6.540664375715922e-06,
      "loss": 0.0012,
      "step": 2340
    },
    {
      "epoch": 2.422680412371134,
      "grad_norm": 1.107264518737793,
      "learning_rate": 6.426116838487972e-06,
      "loss": 0.0478,
      "step": 2350
    },
    {
      "epoch": 2.4329896907216497,
      "grad_norm": 1.89871346950531,
      "learning_rate": 6.311569301260023e-06,
      "loss": 0.0519,
      "step": 2360
    },
    {
      "epoch": 2.443298969072165,
      "grad_norm": 0.03375993296504021,
      "learning_rate": 6.197021764032074e-06,
      "loss": 0.0518,
      "step": 2370
    },
    {
      "epoch": 2.4536082474226806,
      "grad_norm": 7.356297492980957,
      "learning_rate": 6.082474226804124e-06,
      "loss": 0.0507,
      "step": 2380
    },
    {
      "epoch": 2.463917525773196,
      "grad_norm": 5.678482532501221,
      "learning_rate": 5.9679266895761745e-06,
      "loss": 0.0755,
      "step": 2390
    },
    {
      "epoch": 2.4742268041237114,
      "grad_norm": 0.14140386879444122,
      "learning_rate": 5.853379152348224e-06,
      "loss": 0.0506,
      "step": 2400
    },
    {
      "epoch": 2.484536082474227,
      "grad_norm": 0.023605309426784515,
      "learning_rate": 5.7388316151202745e-06,
      "loss": 0.0149,
      "step": 2410
    },
    {
      "epoch": 2.4948453608247423,
      "grad_norm": 0.29622700810432434,
      "learning_rate": 5.624284077892326e-06,
      "loss": 0.0079,
      "step": 2420
    },
    {
      "epoch": 2.5051546391752577,
      "grad_norm": 0.8850248456001282,
      "learning_rate": 5.509736540664376e-06,
      "loss": 0.0445,
      "step": 2430
    },
    {
      "epoch": 2.515463917525773,
      "grad_norm": 0.09807903319597244,
      "learning_rate": 5.395189003436427e-06,
      "loss": 0.0012,
      "step": 2440
    },
    {
      "epoch": 2.5257731958762886,
      "grad_norm": 1.893988013267517,
      "learning_rate": 5.280641466208476e-06,
      "loss": 0.0532,
      "step": 2450
    },
    {
      "epoch": 2.536082474226804,
      "grad_norm": 22.977548599243164,
      "learning_rate": 5.166093928980527e-06,
      "loss": 0.0279,
      "step": 2460
    },
    {
      "epoch": 2.5463917525773194,
      "grad_norm": 0.023369647562503815,
      "learning_rate": 5.051546391752578e-06,
      "loss": 0.0296,
      "step": 2470
    },
    {
      "epoch": 2.556701030927835,
      "grad_norm": 0.014295742847025394,
      "learning_rate": 4.936998854524628e-06,
      "loss": 0.0035,
      "step": 2480
    },
    {
      "epoch": 2.5670103092783503,
      "grad_norm": 0.013844544067978859,
      "learning_rate": 4.822451317296679e-06,
      "loss": 0.0392,
      "step": 2490
    },
    {
      "epoch": 2.5773195876288657,
      "grad_norm": 0.01498400792479515,
      "learning_rate": 4.707903780068728e-06,
      "loss": 0.0058,
      "step": 2500
    },
    {
      "epoch": 2.5876288659793816,
      "grad_norm": 0.648949146270752,
      "learning_rate": 4.593356242840779e-06,
      "loss": 0.0059,
      "step": 2510
    },
    {
      "epoch": 2.597938144329897,
      "grad_norm": 2.189363479614258,
      "learning_rate": 4.478808705612829e-06,
      "loss": 0.0068,
      "step": 2520
    },
    {
      "epoch": 2.6082474226804124,
      "grad_norm": 2.156400203704834,
      "learning_rate": 4.36426116838488e-06,
      "loss": 0.0381,
      "step": 2530
    },
    {
      "epoch": 2.618556701030928,
      "grad_norm": 0.965548038482666,
      "learning_rate": 4.249713631156931e-06,
      "loss": 0.0727,
      "step": 2540
    },
    {
      "epoch": 2.6288659793814433,
      "grad_norm": 0.017810124903917313,
      "learning_rate": 4.13516609392898e-06,
      "loss": 0.04,
      "step": 2550
    },
    {
      "epoch": 2.6391752577319587,
      "grad_norm": 2.8910582065582275,
      "learning_rate": 4.020618556701031e-06,
      "loss": 0.0986,
      "step": 2560
    },
    {
      "epoch": 2.649484536082474,
      "grad_norm": 0.025233469903469086,
      "learning_rate": 3.906071019473081e-06,
      "loss": 0.0545,
      "step": 2570
    },
    {
      "epoch": 2.6597938144329896,
      "grad_norm": 0.05357595905661583,
      "learning_rate": 3.791523482245132e-06,
      "loss": 0.003,
      "step": 2580
    },
    {
      "epoch": 2.670103092783505,
      "grad_norm": 0.03887518495321274,
      "learning_rate": 3.676975945017182e-06,
      "loss": 0.0151,
      "step": 2590
    },
    {
      "epoch": 2.680412371134021,
      "grad_norm": 0.02540469728410244,
      "learning_rate": 3.562428407789233e-06,
      "loss": 0.0397,
      "step": 2600
    },
    {
      "epoch": 2.6907216494845363,
      "grad_norm": 0.09832774847745895,
      "learning_rate": 3.447880870561283e-06,
      "loss": 0.0471,
      "step": 2610
    },
    {
      "epoch": 2.7010309278350517,
      "grad_norm": 0.03695299103856087,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0403,
      "step": 2620
    },
    {
      "epoch": 2.711340206185567,
      "grad_norm": 0.03101181983947754,
      "learning_rate": 3.2187857961053837e-06,
      "loss": 0.0159,
      "step": 2630
    },
    {
      "epoch": 2.7216494845360826,
      "grad_norm": 0.024365022778511047,
      "learning_rate": 3.104238258877434e-06,
      "loss": 0.0221,
      "step": 2640
    },
    {
      "epoch": 2.731958762886598,
      "grad_norm": 0.020094769075512886,
      "learning_rate": 2.989690721649485e-06,
      "loss": 0.0188,
      "step": 2650
    },
    {
      "epoch": 2.7422680412371134,
      "grad_norm": 0.219673752784729,
      "learning_rate": 2.875143184421535e-06,
      "loss": 0.0023,
      "step": 2660
    },
    {
      "epoch": 2.752577319587629,
      "grad_norm": 0.019688084721565247,
      "learning_rate": 2.7605956471935854e-06,
      "loss": 0.0072,
      "step": 2670
    },
    {
      "epoch": 2.7628865979381443,
      "grad_norm": 0.011477293446660042,
      "learning_rate": 2.646048109965636e-06,
      "loss": 0.0396,
      "step": 2680
    },
    {
      "epoch": 2.7731958762886597,
      "grad_norm": 0.015805484727025032,
      "learning_rate": 2.5315005727376863e-06,
      "loss": 0.0595,
      "step": 2690
    },
    {
      "epoch": 2.783505154639175,
      "grad_norm": 0.028426609933376312,
      "learning_rate": 2.4169530355097363e-06,
      "loss": 0.0015,
      "step": 2700
    },
    {
      "epoch": 2.7938144329896906,
      "grad_norm": 0.05143721401691437,
      "learning_rate": 2.302405498281787e-06,
      "loss": 0.0421,
      "step": 2710
    },
    {
      "epoch": 2.804123711340206,
      "grad_norm": 0.02276424504816532,
      "learning_rate": 2.1878579610538375e-06,
      "loss": 0.0382,
      "step": 2720
    },
    {
      "epoch": 2.8144329896907214,
      "grad_norm": 0.7059816718101501,
      "learning_rate": 2.0733104238258875e-06,
      "loss": 0.04,
      "step": 2730
    },
    {
      "epoch": 2.824742268041237,
      "grad_norm": 0.05951162055134773,
      "learning_rate": 1.9587628865979384e-06,
      "loss": 0.0509,
      "step": 2740
    },
    {
      "epoch": 2.8350515463917527,
      "grad_norm": 0.012846922501921654,
      "learning_rate": 1.8442153493699886e-06,
      "loss": 0.0011,
      "step": 2750
    },
    {
      "epoch": 2.845360824742268,
      "grad_norm": 0.02608264423906803,
      "learning_rate": 1.729667812142039e-06,
      "loss": 0.0428,
      "step": 2760
    },
    {
      "epoch": 2.8556701030927836,
      "grad_norm": 0.02047787234187126,
      "learning_rate": 1.6151202749140894e-06,
      "loss": 0.0037,
      "step": 2770
    },
    {
      "epoch": 2.865979381443299,
      "grad_norm": 0.026910128071904182,
      "learning_rate": 1.5005727376861396e-06,
      "loss": 0.0066,
      "step": 2780
    },
    {
      "epoch": 2.8762886597938144,
      "grad_norm": 0.034369904547929764,
      "learning_rate": 1.3860252004581903e-06,
      "loss": 0.0011,
      "step": 2790
    },
    {
      "epoch": 2.88659793814433,
      "grad_norm": 0.028249025344848633,
      "learning_rate": 1.2714776632302407e-06,
      "loss": 0.0119,
      "step": 2800
    },
    {
      "epoch": 2.8969072164948453,
      "grad_norm": 0.30386999249458313,
      "learning_rate": 1.156930126002291e-06,
      "loss": 0.058,
      "step": 2810
    },
    {
      "epoch": 2.9072164948453607,
      "grad_norm": 0.04188067093491554,
      "learning_rate": 1.0423825887743413e-06,
      "loss": 0.0258,
      "step": 2820
    },
    {
      "epoch": 2.917525773195876,
      "grad_norm": 0.05468876287341118,
      "learning_rate": 9.278350515463919e-07,
      "loss": 0.0566,
      "step": 2830
    },
    {
      "epoch": 2.927835051546392,
      "grad_norm": 0.0315389484167099,
      "learning_rate": 8.132875143184422e-07,
      "loss": 0.0055,
      "step": 2840
    },
    {
      "epoch": 2.9381443298969074,
      "grad_norm": 0.6945332884788513,
      "learning_rate": 6.987399770904926e-07,
      "loss": 0.0011,
      "step": 2850
    },
    {
      "epoch": 2.948453608247423,
      "grad_norm": 0.07607989758253098,
      "learning_rate": 5.84192439862543e-07,
      "loss": 0.0082,
      "step": 2860
    },
    {
      "epoch": 2.9587628865979383,
      "grad_norm": 0.31438547372817993,
      "learning_rate": 4.6964490263459333e-07,
      "loss": 0.0513,
      "step": 2870
    },
    {
      "epoch": 2.9690721649484537,
      "grad_norm": 0.026535123586654663,
      "learning_rate": 3.550973654066438e-07,
      "loss": 0.0722,
      "step": 2880
    },
    {
      "epoch": 2.979381443298969,
      "grad_norm": 0.4621647298336029,
      "learning_rate": 2.405498281786941e-07,
      "loss": 0.0803,
      "step": 2890
    },
    {
      "epoch": 2.9896907216494846,
      "grad_norm": 0.03463723137974739,
      "learning_rate": 1.2600229095074457e-07,
      "loss": 0.0015,
      "step": 2900
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.01927581988275051,
      "learning_rate": 1.145475372279496e-08,
      "loss": 0.0408,
      "step": 2910
    },
    {
      "epoch": 3.0,
      "eval_f1": 0.7794117647058824,
      "eval_loss": 0.08493854105472565,
      "eval_precision": 0.8688524590163934,
      "eval_recall": 0.7066666666666667,
      "eval_runtime": 1.9637,
      "eval_samples_per_second": 1692.252,
      "eval_steps_per_second": 105.925,
      "step": 2910
    }
  ],
  "logging_steps": 10,
  "max_steps": 2910,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1540529612669952.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
