2025-11-05 19:11:50 | INFO     | __main__ | Configuration hash: 081f173e86262287
2025-11-05 19:11:50 | INFO     | spot_scam.data.ingest | Loading raw dataset from data/fake_job_postings.csv
2025-11-05 19:11:51 | INFO     | spot_scam.data.ingest | Loading raw dataset from data/Fake_Real_Job_Posting.csv
2025-11-05 19:11:51 | INFO     | spot_scam.data.ingest | Dropped 10414 potential duplicate rows after merging sources.
2025-11-05 19:11:56 | INFO     | spot_scam.data.preprocess | Dropping 6 columns marked for removal: ['_source_file', 'unnamed_0', 'location', 'department', 'job_id', 'salary_range']
2025-11-05 19:11:56 | INFO     | spot_scam.data.split | Creating stratified train/val/test splits (70/15/15)
2025-11-05 19:11:57 | WARNING  | spot_scam.data.split | Detected 3224 duplicate records based on text checksum; dropping duplicates.
2025-11-05 19:11:57 | INFO     | spot_scam.data.split | Persisted split indices to data/processed/split_indices.npz
2025-11-05 19:11:57 | INFO     | spot_scam.data.split | Train split size: 15485 | fraud ratio: 0.045
2025-11-05 19:11:57 | INFO     | spot_scam.data.split | Val split size: 3318 | fraud ratio: 0.045
2025-11-05 19:11:57 | INFO     | spot_scam.data.split | Test split size: 3319 | fraud ratio: 0.045
2025-11-05 19:12:14 | INFO     | spot_scam.models.classical | Logistic Regression (C=0.1) F1=0.588 Precision=0.650 Recall=0.537
2025-11-05 19:12:21 | INFO     | spot_scam.models.classical | Logistic Regression (C=1.0) F1=0.787 Precision=0.890 Recall=0.705
2025-11-05 19:12:31 | INFO     | spot_scam.models.classical | Logistic Regression (C=10.0) F1=0.816 Precision=0.902 Recall=0.745
2025-11-05 19:12:45 | INFO     | spot_scam.models.classical | Linear SVM (C=0.1) F1=0.804 Precision=0.856 Recall=0.758
2025-11-05 19:12:52 | INFO     | spot_scam.models.classical | Linear SVM (C=1.0) F1=0.810 Precision=0.888 Recall=0.745
2025-11-05 19:12:56 | INFO     | spot_scam.models.classical | Linear SVM (C=10.0) F1=0.782 Precision=0.869 Recall=0.711
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002227 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:12:56 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.356 Precision=0.397 Recall=0.322
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001582 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:12:57 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.372 Precision=0.606 Recall=0.268
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001447 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:12:57 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.356 Precision=0.397 Recall=0.322
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001469 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:12:57 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.372 Precision=0.606 Recall=0.268
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001763 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:12:58 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.365 Precision=0.400 Recall=0.336
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001519 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:12:58 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.383 Precision=0.383 Recall=0.383
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001440 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:12:59 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.365 Precision=0.400 Recall=0.336
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001959 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:00 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.383 Precision=0.383 Recall=0.383
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001857 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:00 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.373 Precision=0.377 Recall=0.369
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001509 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:00 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.338 Precision=0.281 Recall=0.423
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001436 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:00 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.373 Precision=0.377 Recall=0.369
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001452 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:01 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.338 Precision=0.281 Recall=0.423
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001427 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:01 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.347 Precision=0.385 Recall=0.315
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:02 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.337 Precision=0.353 Recall=0.322
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001443 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:02 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.347 Precision=0.385 Recall=0.315
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001480 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:03 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.337 Precision=0.353 Recall=0.322
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001436 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:03 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.377 Precision=0.402 Recall=0.356
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001506 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:04 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.354 Precision=0.359 Recall=0.349
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001519 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:04 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.377 Precision=0.402 Recall=0.356
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001544 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:05 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.354 Precision=0.359 Recall=0.349
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001885 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:05 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.344 Precision=0.369 Recall=0.322
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002507 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:06 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.335 Precision=0.464 Recall=0.262
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001938 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:07 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.344 Precision=0.369 Recall=0.322
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001472 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:08 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.05, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.335 Precision=0.464 Recall=0.262
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001649 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:08 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.363 Precision=0.455 Recall=0.302
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001555 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:08 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.8, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.343 Precision=0.366 Recall=0.322
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001490 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:09 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.363 Precision=0.455 Recall=0.302
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001513 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:09 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 1.0, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.343 Precision=0.366 Recall=0.322
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001557 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:10 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.349 Precision=0.477 Recall=0.275
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001525 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:11 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 0.8, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.344 Precision=0.369 Recall=0.322
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001443 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:11 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 0.8, 'class_weight': 'balanced'} F1=0.349 Precision=0.477 Recall=0.275
[LightGBM] [Info] Number of positive: 699, number of negative: 14786
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001482 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 607
[LightGBM] [Info] Number of data points in the train set: 15485, number of used features: 19
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000
[LightGBM] [Info] Start training from score -0.000000
2025-11-05 19:13:12 | INFO     | spot_scam.models.classical | LightGBM {'num_leaves': 63, 'max_depth': -1, 'learning_rate': 0.1, 'n_estimators': 400, 'subsample': 1.0, 'colsample_bytree': 1.0, 'class_weight': 'balanced'} F1=0.344 Precision=0.369 Recall=0.322
